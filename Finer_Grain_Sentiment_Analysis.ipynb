{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NfkreWwXz-z",
        "outputId": "9baf4584-a11a-4cc4-8396-eafa83faabf2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install keras_preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KCkKm90vemqL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, SpatialDropout1D, GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers\n",
        "from keras.utils import to_categorical, pad_sequences\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLIFLCp0hCPO",
        "outputId": "2260aa00-0cc7-4827-be65-cead9a771156"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cZBhJFwxhDTi"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"/content/drive/MyDrive/Movie Research/yelp_reviews.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rah0JelywI6u",
        "outputId": "caa51bda-dfc9-47d1-d692-9c0a0364152a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([3., 5., 4., 1., 2.])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data['stars'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "R3s9RuEghn6i"
      },
      "outputs": [],
      "source": [
        "temp = data.sample(n = 400000, random_state=42)\n",
        "df = temp.sample(frac = .1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZ7B1pvdipqz",
        "outputId": "c7c35d58-b137-4bc3-beda-a9a9b7547213"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uRck8272j4D_"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jW0OIAXfj6Qj"
      },
      "outputs": [],
      "source": [
        "def lemmatize_text(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = nltk.word_tokenize(text)\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return ' '.join(lemmatized_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "xkHC9E-Ij7-c"
      },
      "outputs": [],
      "source": [
        "df['text'] = df['text'].str.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2nXUNIXkGGE",
        "outputId": "12925ef6-fdd5-4e94-c86c-28ab35407a8b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-12-e7256b50f5e4>:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df['text'] = df['text'].str.replace('[^\\w\\s]', '')\n"
          ]
        }
      ],
      "source": [
        "df['text'] = df['text'].str.replace('[^\\w\\s]', '')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "T7YGPjRakHix"
      },
      "outputs": [],
      "source": [
        "# Apply the remove_stopwords and lemmatize_text functions to the 'text' column\n",
        "df['text'] = df['text'].apply(remove_stopwords)\n",
        "df['text'] = df['text'].apply(lemmatize_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "znpR9iHhkJa9"
      },
      "outputs": [],
      "source": [
        "train_df = df[:27000]\n",
        "test_df = df[27000:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "BMgNEIjFkLFy"
      },
      "outputs": [],
      "source": [
        "max_features = 20000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "MTHco1-OkMdc"
      },
      "outputs": [],
      "source": [
        "# Tokenization\n",
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(list(train_df['text'].values))\n",
        "X_train = tokenizer.texts_to_sequences(train_df['text'].values)\n",
        "X_test = tokenizer.texts_to_sequences(test_df['text'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "PzTDfOzvkz-p"
      },
      "outputs": [],
      "source": [
        "x_train = pad_sequences(X_train, maxlen=200)\n",
        "x_test = pad_sequences(X_test, maxlen=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "9HfQVLyVloKc"
      },
      "outputs": [],
      "source": [
        "word_index = tokenizer.word_index\n",
        "nb_words = min(max_features, len(word_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "_mUIHSeIlpjv"
      },
      "outputs": [],
      "source": [
        "embedding_matrix = np.zeros((nb_words, 200))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "hVDfOD5vlqxy"
      },
      "outputs": [],
      "source": [
        "embedding_file = '/content/drive/MyDrive/Movie Research/glove.twitter.27B.200d.txt'\n",
        "\n",
        "# read in embeddings\n",
        "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
        "embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(embedding_file, 'r', encoding = 'utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "92doXnhFnUig"
      },
      "outputs": [],
      "source": [
        "word_index = tokenizer.word_index\n",
        "nb_words = min(max_features, len(word_index))\n",
        "\n",
        "embedding_matrix = np.zeros((nb_words, 200))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ox5mC6eMnXZ1"
      },
      "outputs": [],
      "source": [
        "missed = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "SHa2dhTJpxtv"
      },
      "outputs": [],
      "source": [
        "for word, i in word_index.items():\n",
        "\n",
        "    if i == max_features:\n",
        "        break\n",
        "\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    else:\n",
        "        missed.append(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "0y_Ki9gXp069"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "\n",
        "inp = Input(shape = (200,))\n",
        "x = Embedding(max_features, 200, weights = [embedding_matrix], trainable = True)(inp)\n",
        "x = SpatialDropout1D(0.5)(x)\n",
        "x = Bidirectional(LSTM(60, return_sequences=True))(x)\n",
        "x = Bidirectional(LSTM(60, return_sequences=True))(x)\n",
        "x = Bidirectional(GRU(60, return_sequences=True))(x)\n",
        "avg_pool = GlobalAveragePooling1D()(x)\n",
        "max_pool = GlobalMaxPooling1D()(x)\n",
        "conc = concatenate([avg_pool, max_pool])\n",
        "outp = Dense(5, activation = 'sigmoid')(conc)\n",
        "\n",
        "model = Model(inputs = inp, outputs = outp)\n",
        "# patience is how many epochs to wait to see if val_loss will improve again.\n",
        "earlystop = EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 3)\n",
        "checkpoint = ModelCheckpoint(monitor = 'val_loss', save_best_only = True, filepath = 'yelp_lstm_gru_weights.hdf5')\n",
        "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "y_train = pd.get_dummies(train_df[\"stars\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfxX8-wPqABu",
        "outputId": "e37e23b2-d849-4ea2-8f57-c6378c7dd341"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "48/48 [==============================] - 227s 4s/step - loss: 0.4464 - accuracy: 0.4833 - val_loss: 0.3573 - val_accuracy: 0.5819\n",
            "Epoch 2/10\n",
            "48/48 [==============================] - 213s 4s/step - loss: 0.3443 - accuracy: 0.5888 - val_loss: 0.3122 - val_accuracy: 0.6381\n",
            "Epoch 3/10\n",
            "48/48 [==============================] - 213s 4s/step - loss: 0.3113 - accuracy: 0.6274 - val_loss: 0.2921 - val_accuracy: 0.6552\n",
            "Epoch 4/10\n",
            "48/48 [==============================] - 215s 4s/step - loss: 0.2944 - accuracy: 0.6461 - val_loss: 0.2829 - val_accuracy: 0.6604\n",
            "Epoch 5/10\n",
            "48/48 [==============================] - 215s 4s/step - loss: 0.2827 - accuracy: 0.6637 - val_loss: 0.2770 - val_accuracy: 0.6652\n",
            "Epoch 6/10\n",
            "48/48 [==============================] - 216s 5s/step - loss: 0.2724 - accuracy: 0.6776 - val_loss: 0.2761 - val_accuracy: 0.6659\n",
            "Epoch 7/10\n",
            "48/48 [==============================] - 214s 4s/step - loss: 0.2654 - accuracy: 0.6869 - val_loss: 0.2735 - val_accuracy: 0.6707\n",
            "Epoch 8/10\n",
            "48/48 [==============================] - 216s 5s/step - loss: 0.2572 - accuracy: 0.6952 - val_loss: 0.2760 - val_accuracy: 0.6674\n",
            "Epoch 9/10\n",
            "48/48 [==============================] - 212s 4s/step - loss: 0.2488 - accuracy: 0.7088 - val_loss: 0.2778 - val_accuracy: 0.6693\n",
            "Epoch 10/10\n",
            "48/48 [==============================] - 217s 5s/step - loss: 0.2417 - accuracy: 0.7186 - val_loss: 0.2798 - val_accuracy: 0.6615\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8629d33880>"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(x_train, y_train, batch_size = 512, epochs = 10, validation_split = .1,\n",
        "          callbacks=[earlystop, checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDKPWZl2qB4R",
        "outputId": "9ffe3e73-360d-4d16-a2d1-c06368654252"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 5 of 13). These functions will not be directly callable after loading.\n"
          ]
        }
      ],
      "source": [
        "model.save('saved_model/sentiment_analyzer')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cEnN-zfxsu4",
        "outputId": "64964cdb-dfa3-41ea-b1b4-3f37d201215a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 200)]        0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 200, 200)     4000000     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " spatial_dropout1d (SpatialDrop  (None, 200, 200)    0           ['embedding[0][0]']              \n",
            " out1D)                                                                                           \n",
            "                                                                                                  \n",
            " bidirectional (Bidirectional)  (None, 200, 120)     125280      ['spatial_dropout1d[0][0]']      \n",
            "                                                                                                  \n",
            " bidirectional_1 (Bidirectional  (None, 200, 120)    86880       ['bidirectional[0][0]']          \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " bidirectional_2 (Bidirectional  (None, 200, 120)    65520       ['bidirectional_1[0][0]']        \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " global_average_pooling1d (Glob  (None, 120)         0           ['bidirectional_2[0][0]']        \n",
            " alAveragePooling1D)                                                                              \n",
            "                                                                                                  \n",
            " global_max_pooling1d (GlobalMa  (None, 120)         0           ['bidirectional_2[0][0]']        \n",
            " xPooling1D)                                                                                      \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 240)          0           ['global_average_pooling1d[0][0]'\n",
            "                                                                 , 'global_max_pooling1d[0][0]']  \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 5)            1205        ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,278,885\n",
            "Trainable params: 4,278,885\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "store = tf.keras.models.load_model('saved_model/sentiment_analyzer')\n",
        "store.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E07R6z_Mx2vJ"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GP5KpvH6Edp3",
        "outputId": "5513dcc8-7550-4790-95da-142ccd8d2d4a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<keras.engine.functional.Functional at 0x7f4f2601bfd0>"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDKXyoOZVFFb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
